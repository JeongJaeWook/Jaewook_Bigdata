{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 및 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "#전처리 및 데이터 split\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_label(df=None):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.columns = ['label','C1'] #열 이름 변경\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google = pd.read_excel('data/traindata_google_appstore.xlsm',usecols='B,C')\n",
    "df_brandname = pd.read_excel('data/traindata_car_brandname.xlsx',usecols='B,C')\n",
    "df_comname = pd.read_excel('data/traindata_Company_name.xlsm',usecols='B,C')\n",
    "df_cosme = pd.read_excel('data/traindata_cosmetics.xlsx',usecols='B,C')\n",
    "df_finance = pd.read_excel('data/traindata_finance.xlsx',usecols='B,C')\n",
    "df_obs = pd.read_excel('data/traindata_outback.xlsm',usecols='B,C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [df_google,df_brandname,df_comname,df_cosme,df_finance,df_obs]\n",
    "data_list_labeing = []\n",
    "for df_cat in data_list:\n",
    "    df_cat2 = get_label(df_cat)\n",
    "    data_list_labeing.append(df_cat2)\n",
    "    \n",
    "df = pd.concat(data_list_labeing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5줄만 보기]\n",
      "label 1 : 불량  | 0 : 정상 | 3 : 모름,기억안남,없음\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ㅣㅣㅣㅣ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ㅠㅡㄹ레이스토어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ㅜ글스토어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ㅓㅄ음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>히어로 스카이</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        C1\n",
       "0      1      ㅣㅣㅣㅣ\n",
       "1      1  ㅠㅡㄹ레이스토어\n",
       "2      1     ㅜ글스토어\n",
       "3      1       ㅓㅄ음\n",
       "4      0   히어로 스카이"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[데이터 속성]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 104096 entries, 0 to 6260\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   label   104096 non-null  int64 \n",
      " 1   C1      104096 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n",
      "\n",
      "[데이터 label 갯수]\n",
      "0    101387\n",
      "3      1425\n",
      "1      1284\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n[5줄만 보기]\")\n",
    "print(\"label 1 : 불량  | 0 : 정상 | 3 : 모름,기억안남,없음\")\n",
    "display(df.head(5))\n",
    "\n",
    "print(\"\\n[데이터 속성]\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n[데이터 label 갯수]\")\n",
    "print(df['label'].value_counts())\n",
    "#학습할 불량샘플이 너무 작은게 아닐까..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리\n",
    "클리닝 -> 함수화\n",
    "\n",
    "**모름, 기억나지않음 등과 같은 경우는 어떤 경우엔 정상 샘플로 넘기기 때문에 label을 3으로 표시**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#클리닝 함수\n",
    "def cleanText(text):\n",
    "    repl =''\n",
    "       \n",
    "    if text.isdecimal() : #숫자로만 구성되어 있을 때 공백 치환\n",
    "        text = ''\n",
    "    \n",
    "    text = text.lower() #영어일경우 소문자로 변경\n",
    "    text = text.strip().replace('\\\\','')# 기호 치환\n",
    "    \n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)' # 자음, 모음 제거\n",
    "    text = re.sub(pattern= pattern, repl=repl, string=text)\n",
    "          \n",
    "    pattern = '[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]' # 특수기호 제거\n",
    "    text = re.sub(pattern= pattern, repl=repl, string=text)\n",
    "    \n",
    "    if len(text)==0 :\n",
    "        text = '불량'\n",
    "    #text = text.replace(' ','불량')# 모든 공백 제거 이건 아닌것 같아서 삭제\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인자로 입력받은 DataFrame을 복사 한 뒤 C1 컬럼 문자형 변환하고 복사된 DataFrame 반환\n",
    "\n",
    "\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "               \n",
    "    #1. 응답에 숫자도 있어서 int 형으로 처리됌 -> 문자형으로 변환\n",
    "    df_text = df_copy['C1'].apply(str)   \n",
    "       \n",
    "    #2. 텍스트 클리닝 (자음/모음/숫자만으로 이루어진 경우 빈칸처리, 특수기호 제거)\n",
    "    df_text = df_text.map(lambda x: cleanText(x))\n",
    "    \n",
    "    #3.. 더 있나?\n",
    "        \n",
    "    #마지막에 다시 대입\n",
    "    df_copy['C1re'] = df_text\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = get_preprocessed_df(df)\n",
    "# df_copy.isnull().sum() #공백은 불량으로 처리..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩\n",
    "-단어 수준 임베딩\n",
    "\n",
    "    Latent Semantic Analysis\n",
    "    Word2Vec\n",
    "    GloVe\n",
    "    FastText\n",
    "    Swivel\n",
    "    \n",
    "-문장 수준 임베딩\n",
    "\n",
    "    Weighted Embeddings\n",
    "    Latent Semantic Analysis\n",
    "    Latent Dirichlet Allocation\n",
    "    Doc2Vec\n",
    "    Embeddings from Language Models (ELMo)\n",
    "    Bidirectional Encoder Representations from Transformer (BERT)\n",
    "    \n",
    "    \n",
    "출처 : https://github.com/ratsgo/embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#벡터화\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_features = vectorizer.fit_transform(df_copy['C1'].copy())\n",
    "# df_copy['X_features'] = X_features\n",
    "# y_target = df_copy['label'].copy()\n",
    "# df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#벡터화\n",
    "vectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True, ngram_range=(1,5))\n",
    "def vectorize_tfid_fit(df_copy):    \n",
    "    X_features = vectorizer.fit_transform(df_copy['C1re'].copy())\n",
    "    df_copy['X_features'] = X_features\n",
    "    y_target = df_copy['label'].copy()\n",
    "    return X_features , y_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 데이터를 테스트로\n",
    "X_features, y_target = vectorize_tfid_fit(df_copy)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=333,stratify=y_target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,y_train =  vectorize_tfid_fit(df_copy)\n",
    "# X_test,y_test =  vectorize_tfid_unfit(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's multi_logloss: 0.930253\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's multi_logloss: 0.79833\n",
      "[3]\ttraining's multi_logloss: 0.692454\n",
      "[4]\ttraining's multi_logloss: 0.605687\n",
      "[5]\ttraining's multi_logloss: 0.533576\n",
      "[6]\ttraining's multi_logloss: 0.472707\n",
      "[7]\ttraining's multi_logloss: 0.421477\n",
      "[8]\ttraining's multi_logloss: 0.377678\n",
      "[9]\ttraining's multi_logloss: 0.340122\n",
      "[10]\ttraining's multi_logloss: 0.308022\n",
      "[11]\ttraining's multi_logloss: 0.280338\n",
      "[12]\ttraining's multi_logloss: 0.256293\n",
      "[13]\ttraining's multi_logloss: 0.23544\n",
      "[14]\ttraining's multi_logloss: 0.217127\n",
      "[15]\ttraining's multi_logloss: 0.20087\n",
      "[16]\ttraining's multi_logloss: 0.186836\n",
      "[17]\ttraining's multi_logloss: 0.17426\n",
      "[18]\ttraining's multi_logloss: 0.162977\n",
      "[19]\ttraining's multi_logloss: 0.15312\n",
      "[20]\ttraining's multi_logloss: 0.144301\n",
      "[21]\ttraining's multi_logloss: 0.136391\n",
      "[22]\ttraining's multi_logloss: 0.129455\n",
      "[23]\ttraining's multi_logloss: 0.123249\n",
      "[24]\ttraining's multi_logloss: 0.117537\n",
      "[25]\ttraining's multi_logloss: 0.112383\n",
      "[26]\ttraining's multi_logloss: 0.107757\n",
      "[27]\ttraining's multi_logloss: 0.103483\n",
      "[28]\ttraining's multi_logloss: 0.0996406\n",
      "[29]\ttraining's multi_logloss: 0.0961663\n",
      "[30]\ttraining's multi_logloss: 0.0929797\n",
      "[31]\ttraining's multi_logloss: 0.0900224\n",
      "[32]\ttraining's multi_logloss: 0.0872524\n",
      "[33]\ttraining's multi_logloss: 0.0846867\n",
      "[34]\ttraining's multi_logloss: 0.0823336\n",
      "[35]\ttraining's multi_logloss: 0.0800501\n",
      "[36]\ttraining's multi_logloss: 0.0779217\n",
      "[37]\ttraining's multi_logloss: 0.0759618\n",
      "[38]\ttraining's multi_logloss: 0.0741884\n",
      "[39]\ttraining's multi_logloss: 0.0724566\n",
      "[40]\ttraining's multi_logloss: 0.0708996\n",
      "[41]\ttraining's multi_logloss: 0.069408\n",
      "[42]\ttraining's multi_logloss: 0.0680147\n",
      "[43]\ttraining's multi_logloss: 0.0666711\n",
      "[44]\ttraining's multi_logloss: 0.065413\n",
      "[45]\ttraining's multi_logloss: 0.0642115\n",
      "[46]\ttraining's multi_logloss: 0.0630319\n",
      "[47]\ttraining's multi_logloss: 0.0619348\n",
      "[48]\ttraining's multi_logloss: 0.0609025\n",
      "[49]\ttraining's multi_logloss: 0.0599122\n",
      "[50]\ttraining's multi_logloss: 0.0589974\n",
      "[51]\ttraining's multi_logloss: 0.0580878\n",
      "[52]\ttraining's multi_logloss: 0.0572451\n",
      "[53]\ttraining's multi_logloss: 0.0563919\n",
      "[54]\ttraining's multi_logloss: 0.0556259\n",
      "[55]\ttraining's multi_logloss: 0.0548398\n",
      "[56]\ttraining's multi_logloss: 0.0540131\n",
      "[57]\ttraining's multi_logloss: 0.0533183\n",
      "[58]\ttraining's multi_logloss: 0.0525078\n",
      "[59]\ttraining's multi_logloss: 0.0517984\n",
      "[60]\ttraining's multi_logloss: 0.0511256\n",
      "[61]\ttraining's multi_logloss: 0.0504455\n",
      "[62]\ttraining's multi_logloss: 0.0498102\n",
      "[63]\ttraining's multi_logloss: 0.0491878\n",
      "[64]\ttraining's multi_logloss: 0.0486091\n",
      "[65]\ttraining's multi_logloss: 0.0480221\n",
      "[66]\ttraining's multi_logloss: 0.0474676\n",
      "[67]\ttraining's multi_logloss: 0.0469395\n",
      "[68]\ttraining's multi_logloss: 0.0464782\n",
      "[69]\ttraining's multi_logloss: 0.0460041\n",
      "[70]\ttraining's multi_logloss: 0.0455207\n",
      "[71]\ttraining's multi_logloss: 0.0450415\n",
      "[72]\ttraining's multi_logloss: 0.0445842\n",
      "[73]\ttraining's multi_logloss: 0.0441653\n",
      "[74]\ttraining's multi_logloss: 0.043788\n",
      "[75]\ttraining's multi_logloss: 0.0433718\n",
      "[76]\ttraining's multi_logloss: 0.0429821\n",
      "[77]\ttraining's multi_logloss: 0.0426074\n",
      "[78]\ttraining's multi_logloss: 0.0422406\n",
      "[79]\ttraining's multi_logloss: 0.0419011\n",
      "[80]\ttraining's multi_logloss: 0.0415137\n",
      "[81]\ttraining's multi_logloss: 0.0411793\n",
      "[82]\ttraining's multi_logloss: 0.0408364\n",
      "[83]\ttraining's multi_logloss: 0.0405292\n",
      "[84]\ttraining's multi_logloss: 0.0402558\n",
      "[85]\ttraining's multi_logloss: 0.0399686\n",
      "[86]\ttraining's multi_logloss: 0.0396876\n",
      "[87]\ttraining's multi_logloss: 0.0393995\n",
      "[88]\ttraining's multi_logloss: 0.039106\n",
      "[89]\ttraining's multi_logloss: 0.0388636\n",
      "[90]\ttraining's multi_logloss: 0.0385758\n",
      "[91]\ttraining's multi_logloss: 0.0383374\n",
      "[92]\ttraining's multi_logloss: 0.0381046\n",
      "[93]\ttraining's multi_logloss: 0.0378825\n",
      "[94]\ttraining's multi_logloss: 0.03765\n",
      "[95]\ttraining's multi_logloss: 0.0374141\n",
      "[96]\ttraining's multi_logloss: 0.0371795\n",
      "[97]\ttraining's multi_logloss: 0.0369587\n",
      "[98]\ttraining's multi_logloss: 0.0367361\n",
      "[99]\ttraining's multi_logloss: 0.0365247\n",
      "[100]\ttraining's multi_logloss: 0.0363376\n",
      "[101]\ttraining's multi_logloss: 0.0361642\n",
      "[102]\ttraining's multi_logloss: 0.0359507\n",
      "[103]\ttraining's multi_logloss: 0.0357548\n",
      "[104]\ttraining's multi_logloss: 0.0355815\n",
      "[105]\ttraining's multi_logloss: 0.0353872\n",
      "[106]\ttraining's multi_logloss: 0.035215\n",
      "[107]\ttraining's multi_logloss: 0.0350644\n",
      "[108]\ttraining's multi_logloss: 0.0348892\n",
      "[109]\ttraining's multi_logloss: 0.0346951\n",
      "[110]\ttraining's multi_logloss: 0.0345227\n",
      "[111]\ttraining's multi_logloss: 0.0343663\n",
      "[112]\ttraining's multi_logloss: 0.034208\n",
      "[113]\ttraining's multi_logloss: 0.0340615\n",
      "[114]\ttraining's multi_logloss: 0.0339126\n",
      "[115]\ttraining's multi_logloss: 0.0337546\n",
      "[116]\ttraining's multi_logloss: 0.0336046\n",
      "[117]\ttraining's multi_logloss: 0.0334367\n",
      "[118]\ttraining's multi_logloss: 0.0332956\n",
      "[119]\ttraining's multi_logloss: 0.0331517\n",
      "[120]\ttraining's multi_logloss: 0.0329714\n",
      "[121]\ttraining's multi_logloss: 0.0328266\n",
      "[122]\ttraining's multi_logloss: 0.0327107\n",
      "[123]\ttraining's multi_logloss: 0.0325819\n",
      "[124]\ttraining's multi_logloss: 0.0324633\n",
      "[125]\ttraining's multi_logloss: 0.03235\n",
      "[126]\ttraining's multi_logloss: 0.0322102\n",
      "[127]\ttraining's multi_logloss: 0.0320888\n",
      "[128]\ttraining's multi_logloss: 0.0319573\n",
      "[129]\ttraining's multi_logloss: 0.031844\n",
      "[130]\ttraining's multi_logloss: 0.0317332\n",
      "[131]\ttraining's multi_logloss: 0.0316027\n",
      "[132]\ttraining's multi_logloss: 0.0314947\n",
      "[133]\ttraining's multi_logloss: 0.0313791\n",
      "[134]\ttraining's multi_logloss: 0.0312843\n",
      "[135]\ttraining's multi_logloss: 0.0311733\n",
      "[136]\ttraining's multi_logloss: 0.0310645\n",
      "[137]\ttraining's multi_logloss: 0.0309688\n",
      "[138]\ttraining's multi_logloss: 0.0308774\n",
      "[139]\ttraining's multi_logloss: 0.0307683\n",
      "[140]\ttraining's multi_logloss: 0.0306505\n",
      "[141]\ttraining's multi_logloss: 0.0305632\n",
      "[142]\ttraining's multi_logloss: 0.0304825\n",
      "[143]\ttraining's multi_logloss: 0.0303885\n",
      "[144]\ttraining's multi_logloss: 0.0302885\n",
      "[145]\ttraining's multi_logloss: 0.0302102\n",
      "[146]\ttraining's multi_logloss: 0.030116\n",
      "[147]\ttraining's multi_logloss: 0.0300274\n",
      "[148]\ttraining's multi_logloss: 0.0299318\n",
      "[149]\ttraining's multi_logloss: 0.0298621\n",
      "[150]\ttraining's multi_logloss: 0.0297881\n",
      "[151]\ttraining's multi_logloss: 0.0297073\n",
      "[152]\ttraining's multi_logloss: 0.0296356\n",
      "[153]\ttraining's multi_logloss: 0.0295593\n",
      "[154]\ttraining's multi_logloss: 0.0294797\n",
      "[155]\ttraining's multi_logloss: 0.0293907\n",
      "[156]\ttraining's multi_logloss: 0.0293269\n",
      "[157]\ttraining's multi_logloss: 0.0292561\n",
      "[158]\ttraining's multi_logloss: 0.0291849\n",
      "[159]\ttraining's multi_logloss: 0.0290843\n",
      "[160]\ttraining's multi_logloss: 0.0290194\n",
      "[161]\ttraining's multi_logloss: 0.0289386\n",
      "[162]\ttraining's multi_logloss: 0.0288777\n",
      "[163]\ttraining's multi_logloss: 0.0287977\n",
      "[164]\ttraining's multi_logloss: 0.0287296\n",
      "[165]\ttraining's multi_logloss: 0.0286542\n",
      "[166]\ttraining's multi_logloss: 0.0285854\n",
      "[167]\ttraining's multi_logloss: 0.0285179\n",
      "[168]\ttraining's multi_logloss: 0.0284353\n",
      "[169]\ttraining's multi_logloss: 0.028373\n",
      "[170]\ttraining's multi_logloss: 0.0283217\n",
      "[171]\ttraining's multi_logloss: 0.0282569\n",
      "[172]\ttraining's multi_logloss: 0.0281567\n",
      "[173]\ttraining's multi_logloss: 0.028094\n",
      "[174]\ttraining's multi_logloss: 0.0280296\n",
      "[175]\ttraining's multi_logloss: 0.0279754\n",
      "[176]\ttraining's multi_logloss: 0.0279207\n",
      "[177]\ttraining's multi_logloss: 0.0278746\n",
      "[178]\ttraining's multi_logloss: 0.0277981\n",
      "[179]\ttraining's multi_logloss: 0.027752\n",
      "[180]\ttraining's multi_logloss: 0.0277011\n",
      "[181]\ttraining's multi_logloss: 0.0276361\n",
      "[182]\ttraining's multi_logloss: 0.0275668\n",
      "[183]\ttraining's multi_logloss: 0.0275173\n",
      "[184]\ttraining's multi_logloss: 0.0274617\n",
      "[185]\ttraining's multi_logloss: 0.0274085\n",
      "[186]\ttraining's multi_logloss: 0.027357\n",
      "[187]\ttraining's multi_logloss: 0.0273162\n",
      "[188]\ttraining's multi_logloss: 0.0272617\n",
      "[189]\ttraining's multi_logloss: 0.0272178\n",
      "[190]\ttraining's multi_logloss: 0.0271619\n",
      "[191]\ttraining's multi_logloss: 0.027115\n",
      "[192]\ttraining's multi_logloss: 0.0270714\n",
      "[193]\ttraining's multi_logloss: 0.0270234\n",
      "[194]\ttraining's multi_logloss: 0.0269837\n",
      "[195]\ttraining's multi_logloss: 0.0269362\n",
      "[196]\ttraining's multi_logloss: 0.0268776\n",
      "[197]\ttraining's multi_logloss: 0.0268414\n",
      "[198]\ttraining's multi_logloss: 0.0268009\n",
      "[199]\ttraining's multi_logloss: 0.0267648\n",
      "[200]\ttraining's multi_logloss: 0.0267204\n",
      "[201]\ttraining's multi_logloss: 0.0266833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202]\ttraining's multi_logloss: 0.0266388\n",
      "[203]\ttraining's multi_logloss: 0.0265946\n",
      "[204]\ttraining's multi_logloss: 0.0265467\n",
      "[205]\ttraining's multi_logloss: 0.0264843\n",
      "[206]\ttraining's multi_logloss: 0.026441\n",
      "[207]\ttraining's multi_logloss: 0.0264024\n",
      "[208]\ttraining's multi_logloss: 0.026366\n",
      "[209]\ttraining's multi_logloss: 0.026331\n",
      "[210]\ttraining's multi_logloss: 0.0262923\n",
      "[211]\ttraining's multi_logloss: 0.0262571\n",
      "[212]\ttraining's multi_logloss: 0.0262245\n",
      "[213]\ttraining's multi_logloss: 0.0261907\n",
      "[214]\ttraining's multi_logloss: 0.0261545\n",
      "[215]\ttraining's multi_logloss: 0.0261217\n",
      "[216]\ttraining's multi_logloss: 0.0260781\n",
      "[217]\ttraining's multi_logloss: 0.026041\n",
      "[218]\ttraining's multi_logloss: 0.0260083\n",
      "[219]\ttraining's multi_logloss: 0.02597\n",
      "[220]\ttraining's multi_logloss: 0.0259259\n",
      "[221]\ttraining's multi_logloss: 0.0258951\n",
      "[222]\ttraining's multi_logloss: 0.0258594\n",
      "[223]\ttraining's multi_logloss: 0.0258339\n",
      "[224]\ttraining's multi_logloss: 0.0258005\n",
      "[225]\ttraining's multi_logloss: 0.0257731\n",
      "[226]\ttraining's multi_logloss: 0.0257395\n",
      "[227]\ttraining's multi_logloss: 0.0257008\n",
      "[228]\ttraining's multi_logloss: 0.0256681\n",
      "[229]\ttraining's multi_logloss: 0.0256436\n",
      "[230]\ttraining's multi_logloss: 0.0256135\n",
      "[231]\ttraining's multi_logloss: 0.0255825\n",
      "[232]\ttraining's multi_logloss: 0.0255538\n",
      "[233]\ttraining's multi_logloss: 0.0255163\n",
      "[234]\ttraining's multi_logloss: 0.025479\n",
      "[235]\ttraining's multi_logloss: 0.0254067\n",
      "[236]\ttraining's multi_logloss: 0.0253772\n",
      "[237]\ttraining's multi_logloss: 0.0253508\n",
      "[238]\ttraining's multi_logloss: 0.0253239\n",
      "[239]\ttraining's multi_logloss: 0.0252888\n",
      "[240]\ttraining's multi_logloss: 0.0252612\n",
      "[241]\ttraining's multi_logloss: 0.0252363\n",
      "[242]\ttraining's multi_logloss: 0.025213\n",
      "[243]\ttraining's multi_logloss: 0.0251949\n",
      "[244]\ttraining's multi_logloss: 0.0251618\n",
      "[245]\ttraining's multi_logloss: 0.0251339\n",
      "[246]\ttraining's multi_logloss: 0.0250969\n",
      "[247]\ttraining's multi_logloss: 0.0250763\n",
      "[248]\ttraining's multi_logloss: 0.0250557\n",
      "[249]\ttraining's multi_logloss: 0.0250359\n",
      "[250]\ttraining's multi_logloss: 0.0250125\n",
      "[251]\ttraining's multi_logloss: 0.0249894\n",
      "[252]\ttraining's multi_logloss: 0.0249619\n",
      "[253]\ttraining's multi_logloss: 0.0249408\n",
      "[254]\ttraining's multi_logloss: 0.0249169\n",
      "[255]\ttraining's multi_logloss: 0.0248932\n",
      "[256]\ttraining's multi_logloss: 0.024862\n",
      "[257]\ttraining's multi_logloss: 0.0248442\n",
      "[258]\ttraining's multi_logloss: 0.0248206\n",
      "[259]\ttraining's multi_logloss: 0.0248047\n",
      "[260]\ttraining's multi_logloss: 0.0247834\n",
      "[261]\ttraining's multi_logloss: 0.0247617\n",
      "[262]\ttraining's multi_logloss: 0.0247402\n",
      "[263]\ttraining's multi_logloss: 0.024717\n",
      "[264]\ttraining's multi_logloss: 0.0246963\n",
      "[265]\ttraining's multi_logloss: 0.0246715\n",
      "[266]\ttraining's multi_logloss: 0.0246492\n",
      "[267]\ttraining's multi_logloss: 0.024625\n",
      "[268]\ttraining's multi_logloss: 0.0246073\n",
      "[269]\ttraining's multi_logloss: 0.0245887\n",
      "[270]\ttraining's multi_logloss: 0.0245695\n",
      "[271]\ttraining's multi_logloss: 0.0245546\n",
      "[272]\ttraining's multi_logloss: 0.0245332\n",
      "[273]\ttraining's multi_logloss: 0.0245118\n",
      "[274]\ttraining's multi_logloss: 0.0244963\n",
      "[275]\ttraining's multi_logloss: 0.0244801\n",
      "[276]\ttraining's multi_logloss: 0.0244626\n",
      "[277]\ttraining's multi_logloss: 0.0244457\n",
      "[278]\ttraining's multi_logloss: 0.0244296\n",
      "[279]\ttraining's multi_logloss: 0.0244102\n",
      "[280]\ttraining's multi_logloss: 0.0243944\n",
      "[281]\ttraining's multi_logloss: 0.0243684\n",
      "[282]\ttraining's multi_logloss: 0.024355\n",
      "[283]\ttraining's multi_logloss: 0.0243413\n",
      "[284]\ttraining's multi_logloss: 0.024327\n",
      "[285]\ttraining's multi_logloss: 0.0243044\n",
      "[286]\ttraining's multi_logloss: 0.0242902\n",
      "[287]\ttraining's multi_logloss: 0.0242764\n",
      "[288]\ttraining's multi_logloss: 0.0242598\n",
      "[289]\ttraining's multi_logloss: 0.0242465\n",
      "[290]\ttraining's multi_logloss: 0.0242329\n",
      "[291]\ttraining's multi_logloss: 0.0242197\n",
      "[292]\ttraining's multi_logloss: 0.024199\n",
      "[293]\ttraining's multi_logloss: 0.0241752\n",
      "[294]\ttraining's multi_logloss: 0.0241625\n",
      "[295]\ttraining's multi_logloss: 0.0241468\n",
      "[296]\ttraining's multi_logloss: 0.0241309\n",
      "[297]\ttraining's multi_logloss: 0.0241125\n",
      "[298]\ttraining's multi_logloss: 0.0240942\n",
      "[299]\ttraining's multi_logloss: 0.0240776\n",
      "[300]\ttraining's multi_logloss: 0.0240648\n",
      "[301]\ttraining's multi_logloss: 0.0240527\n",
      "[302]\ttraining's multi_logloss: 0.0240366\n",
      "[303]\ttraining's multi_logloss: 0.0240208\n",
      "[304]\ttraining's multi_logloss: 0.0240083\n",
      "[305]\ttraining's multi_logloss: 0.0239966\n",
      "[306]\ttraining's multi_logloss: 0.0239808\n",
      "[307]\ttraining's multi_logloss: 0.02397\n",
      "[308]\ttraining's multi_logloss: 0.0239563\n",
      "[309]\ttraining's multi_logloss: 0.0239437\n",
      "[310]\ttraining's multi_logloss: 0.0239305\n",
      "[311]\ttraining's multi_logloss: 0.0239195\n",
      "[312]\ttraining's multi_logloss: 0.0239098\n",
      "[313]\ttraining's multi_logloss: 0.0239\n",
      "[314]\ttraining's multi_logloss: 0.0238893\n",
      "[315]\ttraining's multi_logloss: 0.0238789\n",
      "[316]\ttraining's multi_logloss: 0.0238499\n",
      "[317]\ttraining's multi_logloss: 0.0238383\n",
      "[318]\ttraining's multi_logloss: 0.0238275\n",
      "[319]\ttraining's multi_logloss: 0.0238174\n",
      "[320]\ttraining's multi_logloss: 0.0238062\n",
      "[321]\ttraining's multi_logloss: 0.023796\n",
      "[322]\ttraining's multi_logloss: 0.0237865\n",
      "[323]\ttraining's multi_logloss: 0.023774\n",
      "[324]\ttraining's multi_logloss: 0.023765\n",
      "[325]\ttraining's multi_logloss: 0.0237538\n",
      "[326]\ttraining's multi_logloss: 0.0237407\n",
      "[327]\ttraining's multi_logloss: 0.0237295\n",
      "[328]\ttraining's multi_logloss: 0.0237193\n",
      "[329]\ttraining's multi_logloss: 0.0237102\n",
      "[330]\ttraining's multi_logloss: 0.0236998\n",
      "[331]\ttraining's multi_logloss: 0.023685\n",
      "[332]\ttraining's multi_logloss: 0.0236756\n",
      "[333]\ttraining's multi_logloss: 0.023664\n",
      "[334]\ttraining's multi_logloss: 0.0236524\n",
      "[335]\ttraining's multi_logloss: 0.023641\n",
      "[336]\ttraining's multi_logloss: 0.0236307\n",
      "[337]\ttraining's multi_logloss: 0.0236187\n",
      "[338]\ttraining's multi_logloss: 0.0236092\n",
      "[339]\ttraining's multi_logloss: 0.0236\n",
      "[340]\ttraining's multi_logloss: 0.0235899\n",
      "[341]\ttraining's multi_logloss: 0.02358\n",
      "[342]\ttraining's multi_logloss: 0.0235652\n",
      "[343]\ttraining's multi_logloss: 0.0235581\n",
      "[344]\ttraining's multi_logloss: 0.0235468\n",
      "[345]\ttraining's multi_logloss: 0.0235391\n",
      "[346]\ttraining's multi_logloss: 0.0235266\n",
      "[347]\ttraining's multi_logloss: 0.0235183\n",
      "[348]\ttraining's multi_logloss: 0.0235098\n",
      "[349]\ttraining's multi_logloss: 0.0235\n",
      "[350]\ttraining's multi_logloss: 0.0234912\n",
      "[351]\ttraining's multi_logloss: 0.0234843\n",
      "[352]\ttraining's multi_logloss: 0.0234767\n",
      "[353]\ttraining's multi_logloss: 0.0234692\n",
      "[354]\ttraining's multi_logloss: 0.0234598\n",
      "[355]\ttraining's multi_logloss: 0.0234505\n",
      "[356]\ttraining's multi_logloss: 0.0234423\n",
      "[357]\ttraining's multi_logloss: 0.0234346\n",
      "[358]\ttraining's multi_logloss: 0.0234249\n",
      "[359]\ttraining's multi_logloss: 0.0234174\n",
      "[360]\ttraining's multi_logloss: 0.0234078\n",
      "[361]\ttraining's multi_logloss: 0.0234006\n",
      "[362]\ttraining's multi_logloss: 0.0233904\n",
      "[363]\ttraining's multi_logloss: 0.0233809\n",
      "[364]\ttraining's multi_logloss: 0.0233743\n",
      "[365]\ttraining's multi_logloss: 0.0233668\n",
      "[366]\ttraining's multi_logloss: 0.0233584\n",
      "[367]\ttraining's multi_logloss: 0.0233509\n",
      "[368]\ttraining's multi_logloss: 0.023342\n",
      "[369]\ttraining's multi_logloss: 0.0233345\n",
      "[370]\ttraining's multi_logloss: 0.0233281\n",
      "[371]\ttraining's multi_logloss: 0.0233188\n",
      "[372]\ttraining's multi_logloss: 0.0233124\n",
      "[373]\ttraining's multi_logloss: 0.0233041\n",
      "[374]\ttraining's multi_logloss: 0.0232975\n",
      "[375]\ttraining's multi_logloss: 0.0232905\n",
      "[376]\ttraining's multi_logloss: 0.0232806\n",
      "[377]\ttraining's multi_logloss: 0.023274\n",
      "[378]\ttraining's multi_logloss: 0.0232648\n",
      "[379]\ttraining's multi_logloss: 0.0232551\n",
      "[380]\ttraining's multi_logloss: 0.0232477\n",
      "[381]\ttraining's multi_logloss: 0.023241\n",
      "[382]\ttraining's multi_logloss: 0.0232363\n",
      "[383]\ttraining's multi_logloss: 0.0232299\n",
      "[384]\ttraining's multi_logloss: 0.0232235\n",
      "[385]\ttraining's multi_logloss: 0.023219\n",
      "[386]\ttraining's multi_logloss: 0.0232115\n",
      "[387]\ttraining's multi_logloss: 0.0232057\n",
      "[388]\ttraining's multi_logloss: 0.0232013\n",
      "[389]\ttraining's multi_logloss: 0.0231926\n",
      "[390]\ttraining's multi_logloss: 0.0231866\n",
      "[391]\ttraining's multi_logloss: 0.023179\n",
      "[392]\ttraining's multi_logloss: 0.0231723\n",
      "[393]\ttraining's multi_logloss: 0.023166\n",
      "[394]\ttraining's multi_logloss: 0.0231595\n",
      "[395]\ttraining's multi_logloss: 0.0231511\n",
      "[396]\ttraining's multi_logloss: 0.0231461\n",
      "[397]\ttraining's multi_logloss: 0.0231408\n",
      "[398]\ttraining's multi_logloss: 0.0231363\n",
      "[399]\ttraining's multi_logloss: 0.023131\n",
      "[400]\ttraining's multi_logloss: 0.0231247\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttraining's multi_logloss: 0.0231247\n"
     ]
    }
   ],
   "source": [
    "# LightGBM의 파이썬 패키지인 lightgbm에서 LGBMClassifier 임포트\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 앞서 XGBoost와 동일하게 n_estimators는 400 설정. \n",
    "lgbm_wrapper = LGBMClassifier(n_estimators=400,class_weight = 'balanced', )\n",
    "\n",
    "# LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능. \n",
    "evals = [(X_features, y_target)]\n",
    "lgbm_wrapper.fit(X_features, y_target, early_stopping_rounds=100, eval_metric=\"logloss\", \n",
    "                 eval_set=evals, verbose=True)\n",
    "preds = lgbm_wrapper.predict(X_test)\n",
    "pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred, pos_label='positive', average='micro')\n",
    "    recall = recall_score(y_test , pred, pos_label='positive', average='micro')\n",
    "    f1 = f1_score(y_test,pred, average='micro')\n",
    "    # ROC-AUC 추가 \n",
    "#     roc_auc = roc_auc_score(y_test, pred_proba, average='micro' , multi_class = 'ovo')\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    \n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}'.format(accuracy, precision, recall, f1))\n",
    "    \n",
    "#     # ROC-AUC print 추가\n",
    "#     print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "#     F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차 행렬\n",
      "[[20246    32     0]\n",
      " [    8   248     1]\n",
      " [    0     0   285]]\n",
      "정확도: 0.9980, 정밀도: 0.9980, 재현율: 0.9980,    F1: 0.9980\n"
     ]
    }
   ],
   "source": [
    "get_clf_eval(y_test, preds, pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "with open('model_google.pkl', 'wb') as file:  \n",
    "    pickle.dump(lgbm_wrapper, file)\n",
    "\n",
    "#벡터라이즈도 저장해줘야함!!!!\n",
    "with open('TVectorizer.pkl', 'wb') as file:  \n",
    "    pickle.dump(vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
